"""
Gemini AI í´ë¼ì´ì–¸íŠ¸
Vertex AI ê¸°ë°˜ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ìƒì„±
"""

import asyncio
import json
import re
import time
from typing import Any, Callable, Optional

from config.constants import HOOK_TEMPLATES, HOOK_TYPES
from core.prompts import prompt_registry
from core.prompts import marketing_prompts  # noqa: F401
from core.exceptions import GeminiAPIError
from utils.logger import get_logger, log_llm_fail, log_llm_request, log_llm_response
from utils.cache import cached
from utils.retry import retry_on_error

logger = get_logger(__name__)


class GeminiClient:
    """Gemini AI í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        project_id: str,
        location: str,
        text_model: str = "gemini-3-pro-preview",
        image_model: str = "gemini-3-pro-image-preview",
    ) -> None:
        self._project_id = project_id
        self._location = location
        self._text_model = text_model
        self._image_model = image_model
        self._client = None
        self._async_client = None
        self._async_client_loop = None

    def _get_client(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ì´ˆê¸°í™”)"""
        if self._client is None:
            from google import genai

            self._client = genai.Client(
                vertexai=True,
                project=self._project_id,
                location=self._location,
            )
        return self._client

    def is_configured(self) -> bool:
        """ì„¤ì • í™•ì¸"""
        return bool(self._project_id and self._location)

    def health_check(self) -> bool:
        """API ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            self._get_client()
            return True
        except Exception:
            return False

    def _get_async_client(self):
        """ë¹„ë™ê¸° Gemini í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        from google import genai

        return genai.Client(
            vertexai=True,
            project=self._project_id,
            location=self._location,
        )

    async def generate_content_async(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_retries: int = 3,
    ) -> str:
        """ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„± (sync í˜¸ì¶œì„ threadë¡œ ê°ì‹¸ê¸°)"""
        last_error = None
        for attempt in range(max_retries):
            try:
                return await asyncio.to_thread(self.generate_text, prompt, temperature)
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                is_rate_limit = any(
                    keyword in error_str for keyword in ["429", "rate limit", "quota"]
                )
                if is_rate_limit and attempt < max_retries - 1:
                    delay = min(1.0 * (2**attempt), 10.0)
                    logger.warning(
                        f"Rate limit, {delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})"
                    )
                    await asyncio.sleep(delay)
                else:
                    break

        log_llm_fail("í…ìŠ¤íŠ¸ ìƒì„±(ë¹„ë™ê¸°)", str(last_error))
        return ""

    @cached(ttl=3600, cache_key_prefix="gemini")
    @retry_on_error(max_attempts=3, base_delay=1.0, max_delay=8.0)
    def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        use_grounding: bool = False,
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ì¬ì‹œë„ ë¡œì§ ì ìš©)"""
        import time as _time
        start_time = _time.time()
        
        log_llm_request(
            "í…ìŠ¤íŠ¸ ìƒì„±",
            details=f"temperature={temperature}, grounding={use_grounding}",
            model=self._text_model,
            prompt_preview=prompt,
        )
        try:
            from google.genai import types

            client = self._get_client()

            config = types.GenerateContentConfig(temperature=temperature)

            if use_grounding:
                config.tools = [types.Tool(google_search=types.GoogleSearch())]

            # [AI Product Pattern] Retry Mechanism
            def _api_call():
                return client.models.generate_content(
                    model=self._text_model,
                    contents=prompt,
                    config=config,
                )

            response = self.retry_with_backoff(_api_call)
            elapsed_ms = (_time.time() - start_time) * 1000
            
            response_text = response.text if response and response.text else ""
            log_llm_response(
                "í…ìŠ¤íŠ¸ ìƒì„±",
                details=f"ì‘ë‹µ {len(response_text)}ì",
                response_preview=response_text,
                duration_ms=elapsed_ms,
            )
            return response_text

        except Exception as e:
            log_llm_fail("í…ìŠ¤íŠ¸ ìƒì„±", str(e), model=self._text_model)
            raise GeminiAPIError(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

    @retry_on_error(max_attempts=3, base_delay=1.0, max_delay=8.0)
    def generate_image(
        self,
        prompt: str,
        aspect_ratio: str = "16:9",
    ) -> bytes | None:
        """ì´ë¯¸ì§€ ìƒì„± (Retry ì ìš©) (genesis_kr/v3 ë°©ì‹: generate_content + response_modalities)"""
        import time as _time
        start_time = _time.time()
        
        log_llm_request(
            "ì´ë¯¸ì§€ ìƒì„±",
            details=f"aspect_ratio={aspect_ratio}",
            model=self._image_model,
            prompt_preview=prompt,
        )
        try:
            import base64

            from google.genai.types import GenerateContentConfig, Modality

            client = self._get_client()

            # [AI Product Pattern] Retry Mechanism
            def _api_call():
                return client.models.generate_content(
                    model=self._image_model,
                    contents=prompt,
                    config=GenerateContentConfig(
                        response_modalities=[Modality.TEXT, Modality.IMAGE],
                    ),
                )

            response = self.retry_with_backoff(_api_call)
            elapsed_ms = (_time.time() - start_time) * 1000

            # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì‘ë‹µ ì²˜ë¦¬
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.inline_data:
                        img_data = part.inline_data.data
                        if isinstance(img_data, str):
                            img_data = base64.b64decode(img_data)
                        log_llm_response(
                            "ì´ë¯¸ì§€ ìƒì„±",
                            details=f"{len(img_data):,} bytes ì´ë¯¸ì§€ ìƒì„±ë¨",
                            duration_ms=elapsed_ms,
                        )
                        return img_data

            log_llm_fail("ì´ë¯¸ì§€ ìƒì„±", "ì‘ë‹µì— ì´ë¯¸ì§€ ì—†ìŒ", model=self._image_model)
            return None

        except Exception as e:
            log_llm_fail("ì´ë¯¸ì§€ ìƒì„±", str(e), model=self._image_model)
            raise GeminiAPIError(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

    @cached(ttl=7200, cache_key_prefix="gemini")
    @retry_on_error(max_attempts=3, base_delay=1.0, max_delay=8.0)
    def analyze_marketing_data(
        self,
        youtube_data: dict,
        naver_data: dict,
        product_name: str,
        top_insights: list[dict] = None,
        market_trends: dict | None = None,
        progress_callback: Optional[Callable[[str, int], None]] = None,
        use_search_grounding: bool = True,
    ) -> dict[str, Any]:
        """ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ (Retry & Validation ê°•í™”)"""
        import time as _time
        start_time = _time.time()

        try:
            from google.genai import types

            client = self._get_client()

            if progress_callback:
                progress_callback("ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ì¤‘...", 20)

            analysis_prompt = prompt_registry.get("marketing.analysis").render(
                product_name=product_name,
                top_insights_json=(
                    json.dumps(top_insights, ensure_ascii=False, indent=2)
                    if top_insights
                    else "ë°ì´í„° ì—†ìŒ"
                ),
                market_trends_json=(
                    json.dumps(market_trends, ensure_ascii=False, indent=2)
                    if market_trends
                    else "ë°ì´í„° ì—†ìŒ"
                ),
                youtube_data_json=(
                    json.dumps(youtube_data, ensure_ascii=False, indent=2)
                    if youtube_data
                    else "ë°ì´í„° ì—†ìŒ"
                ),
                naver_data_json=(
                    json.dumps(naver_data, ensure_ascii=False, indent=2)
                    if naver_data
                    else "ë°ì´í„° ì—†ìŒ"
                ),
            )

            log_llm_request(
                "ë§ˆì¼€íŒ… ë¶„ì„",
                details=f"ì œí’ˆ: {product_name}, grounding={use_search_grounding}",
                model=self._text_model,
                prompt_preview=analysis_prompt,
            )

            if progress_callback:
                progress_callback("AI ë¶„ì„ ì§„í–‰ ì¤‘...", 50)

            config = types.GenerateContentConfig(
                temperature=0.7,
                response_mime_type="application/json",
            )

            if use_search_grounding:
                config.tools = [types.Tool(google_search=types.GoogleSearch())]

            # [AI Product Pattern] Retry Mechanism
            def _api_call():
                return client.models.generate_content(
                    model=self._text_model,
                    contents=analysis_prompt,
                    config=config,
                )

            response = self.retry_with_backoff(_api_call)
            elapsed_ms = (_time.time() - start_time) * 1000

            if progress_callback:
                progress_callback("ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì¤‘...", 80)

            result = self._validate_json_output(response.text)
            response_text = response.text if response and response.text else ""
            
            log_llm_response(
                "ë§ˆì¼€íŒ… ë¶„ì„",
                details=f"JSON {len(response_text)}ì ìˆ˜ì‹ ",
                response_preview=response_text,
                duration_ms=elapsed_ms,
            )

            if progress_callback:
                progress_callback("ë¶„ì„ ì™„ë£Œ!", 100)

            return result

        except Exception as e:
            log_llm_fail("ë§ˆì¼€íŒ… ë¶„ì„", str(e), model=self._text_model)
            if progress_callback:
                progress_callback(f"ì˜¤ë¥˜: {e}", 0)
            return {"error": str(e)}

    def generate_marketing_strategy(
        self,
        collected_data: dict,
        progress_callback: Optional[Callable[[str, int], None]] = None,
    ) -> dict[str, Any]:
        """ë§ˆì¼€íŒ… ì „ëµ ìƒì„±"""
        product = collected_data.get("product", {})
        product_name = product.get("name", "ì œí’ˆ")

        youtube_data = collected_data.get("youtube_data", {})
        naver_data = collected_data.get("naver_data", {})
        top_insights = collected_data.get("top_insights", [])
        market_trends = collected_data.get("market_trends")

        return self.analyze_marketing_data(
            youtube_data=youtube_data,
            naver_data=naver_data,
            product_name=product_name,
            top_insights=top_insights,
            market_trends=market_trends,
            progress_callback=progress_callback,
            use_search_grounding=True,
        )

    def _build_image_prompt(
        self,
        product: dict,
        hook_text: str,
        style: str = "ë“œë¼ë§ˆí‹±",
        color_scheme: str = "ë¸”ë£¨ ê·¸ë¼ë””ì–¸íŠ¸",
        layout: str = "ì¤‘ì•™ ì§‘ì¤‘í˜•",
        style_modifier: Optional[str] = None,
        aspect_ratio: str = "16:9",
    ) -> str:
        """ë§ˆì¼€íŒ… ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ë¹Œë“œ"""
        product_name = product.get("name", "ì œí’ˆ")
        product_category = product.get("category", "ì¼ë°˜")

        prompt = f"""### ğŸ¤– Role: Premium E-commerce Visual Designer
You are an expert commercial photographer and digital artist specializing in high-conversion e-commerce imagery.
Your mission: Create a thumbnail that stops the scroll and drives immediate purchase intent.

### ğŸ¯ Product Context
- **Product Name:** {product_name}
- **Category:** {product_category}
- **Hook Text:** "{hook_text}"

### ğŸ¨ Visual Direction
- **Style:** {style} with high-end commercial quality
- **Color Scheme:** {color_scheme}
- **Layout:** {layout}
"""
        if style_modifier:
            prompt += f"- **Style Modifier:** {style_modifier}\n"

        prompt += f"""
### ğŸ“ Composition Requirements
- **Hero Element:** Product must be the undeniable focal point
- **Background:** Clean, uncluttered, complementary to product colors
- **Lighting:** Dramatic studio lighting with soft, professional shadows
- **Depth:** Subtle depth-of-field to separate product from background
- **Safe Zone:** Leave 10% margin on all edges for platform UI overlay

### âœï¸ Text Overlay (CRITICAL)
- **Text Content:** "{hook_text}"
- **Placement:** Prominent, readable at thumbnail size
- **Typography:** Modern, bold sans-serif (no script or thin fonts)
- **Contrast:** High contrast against background (use text shadow or backing)
- **Size:** Large enough to read on mobile (at least 15% of image height)

### ğŸ”§ Technical Specifications
- **Resolution:** 8K quality, ultra-sharp details
- **Aspect Ratio:** {aspect_ratio}
- **Render Style:** Photorealistic with subtle enhancement
- **Color Profile:** Vibrant but natural, Instagram-ready

### ğŸ’ Mood & Emotion
- **Premium Feel:** Luxurious, trustworthy, professional craftsmanship
- **Urgency:** Visual cues that create FOMO (limited, exclusive vibe)
- **Desire:** Make viewers imagine owning this product
- **Action:** Subtle visual flow guiding eye to key elements

### â›” Negative Constraints (AVOID)
- NO watermarks, logos, or brand identifiers (unless requested)
- NO cluttered or busy backgrounds
- NO unrealistic or distorted product proportions
- NO low-quality textures or blurry elements
- NO generic stock photo aesthetics
"""
        return prompt.strip()

    def generate_thumbnail(
        self,
        product: dict,
        hook_text: str,
        style: str = "ë“œë¼ë§ˆí‹±",
        style_modifier: Optional[str] = None,
        aspect_ratio: str = "16:9",
        progress_callback: Optional[Callable[[str, int], None]] = None,
    ) -> bytes | None:
        """ë§ˆì¼€íŒ… ì¸ë„¤ì¼ ìƒì„±"""
        import time as _time
        start_time = _time.time()
        p_name = product.get("name", "N/A")

        try:
            if progress_callback:
                progress_callback("í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì¤‘...", 10)

            prompt = self._build_image_prompt(
                product,
                hook_text,
                style,
                style_modifier=style_modifier,
                aspect_ratio=aspect_ratio,
            )

            log_llm_request(
                "ì¸ë„¤ì¼ ìƒì„±",
                details=f"ì œí’ˆ: {p_name}, ìŠ¤íƒ€ì¼: {style}, ë¹„ìœ¨: {aspect_ratio}",
                model=self._image_model,
                prompt_preview=prompt,
            )

            if progress_callback:
                progress_callback("ì´ë¯¸ì§€ ìƒì„± ì¤‘...", 30)

            image_data = self.generate_image(prompt, aspect_ratio=aspect_ratio)

            if progress_callback:
                progress_callback("ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...", 80)

            elapsed_ms = (_time.time() - start_time) * 1000

            if image_data:
                log_llm_response(
                    "ì¸ë„¤ì¼ ìƒì„±",
                    details=f"{len(image_data):,} bytes ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ",
                    duration_ms=elapsed_ms,
                )
                if progress_callback:
                    progress_callback("ì¸ë„¤ì¼ ì¤€ë¹„ ì™„ë£Œ!", 100)
                return image_data

            return None

        except Exception as e:
            log_llm_fail("ì¸ë„¤ì¼ ìƒì„±", str(e), model=self._image_model)
            if progress_callback:
                progress_callback(f"ì˜¤ë¥˜: {e}", 0)
            return None

    def generate_multiple_thumbnails(
        self,
        product: dict,
        hook_texts: list[str],
        styles: list[str] | None = None,
        progress_callback: Optional[Callable[[str, int], None]] = None,
    ) -> list[dict]:
        """ë‹¤ì¤‘ ì¸ë„¤ì¼ ìƒì„±"""
        p_name = product.get("name", "N/A")
        log_llm_request("ë‹¤ì¤‘ ì¸ë„¤ì¼ ìƒì„±", f"ì œí’ˆ: {p_name}, {len(hook_texts)}ê°œ")

        if styles is None:
            styles = ["ë„¤ì˜¤ë¸Œë£¨íƒˆë¦¬ì¦˜", "ë¹„ë¹„ë“œ", "ëˆ„ì•„ë¥´"]

        results = []
        total = len(hook_texts)

        for i, hook_text in enumerate(hook_texts):
            style = styles[i % len(styles)]

            if progress_callback:
                progress = int((i / total) * 100)
                progress_callback(f"ì¸ë„¤ì¼ {i + 1}/{total} ìƒì„± ì¤‘...", progress)

            image = self.generate_thumbnail(product, hook_text, style)

            if image:
                results.append(
                    {
                        "image": image,
                        "hook_text": hook_text,
                        "style": style,
                    }
                )

        log_llm_response("ë‹¤ì¤‘ ì¸ë„¤ì¼ ìƒì„±", f"{len(results)}/{total}ê°œ ì™„ë£Œ")

        if progress_callback:
            progress_callback("ëª¨ë“  ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ!", 100)

        return results

    def generate_hook_texts(
        self,
        product_name: str,
        hook_types: list[str] | None = None,
        count: int = 5,
        custom_params: dict | None = None,
    ) -> list[dict]:
        """ì‹¬ë¦¬í•™ ê¸°ë°˜ í›… í…ìŠ¤íŠ¸ ìƒì„±"""
        if hook_types is None:
            hook_types = HOOK_TYPES

        params = {
            "product": product_name,
            "count": "10ë§Œ",
            "discount": "50",
            "benefit": "íš¨ê³¼",
            **(custom_params or {}),
        }

        hooks = []
        for hook_type in hook_types:
            if hook_type in HOOK_TEMPLATES:
                for template in HOOK_TEMPLATES[hook_type]:
                    try:
                        hook = template.format(**params)
                        hooks.append({"text": hook, "type": hook_type})
                    except KeyError:
                        continue

        # ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ
        seen = set()
        unique_hooks = []
        for h in hooks:
            if h["text"] not in seen:
                seen.add(h["text"])
                unique_hooks.append(h)
                if len(unique_hooks) >= count:
                    break

        return unique_hooks

    def _extract_first_json_object(self, text: str) -> Optional[str]:
        """ì²« ë²ˆì§¸ ì™„ì „í•œ { ... } ë¸”ë¡ë§Œ ì¶”ì¶œ (ì¤‘ì²© ê´„í˜¸ ëŒ€ì‘)."""
        start = text.find("{")
        if start == -1:
            return None
        depth = 0
        for i in range(start, len(text)):
            if text[i] == "{":
                depth += 1
            elif text[i] == "}":
                depth -= 1
                if depth == 0:
                    return text[start : i + 1]
        return None

    def _fix_common_json_issues(self, raw: str) -> str:
        """LLMì´ ìì£¼ ë‚´ëŠ” JSON ì˜¤ë¥˜ ë³´ì • (ë ì‰¼í‘œ ë“±)."""
        raw = re.sub(r",\s*}", "}", raw)
        raw = re.sub(r",\s*]", "]", raw)
        return raw

    def _validate_json_output(
        self,
        text: str,
        required_fields: list[str] | None = None,
    ) -> dict:
        """LLM ì¶œë ¥ JSON ê²€ì¦ ë° ì •í™”"""
        # [AI Product Pattern] Output Sanitization
        text = re.sub(r"```json\s*", "", text)
        text = re.sub(r"```\s*", "", text)
        text = text.strip()

        try:
            result = json.loads(text)
        except json.JSONDecodeError:
            json_str = self._extract_first_json_object(text)
            if json_str:
                fixed = self._fix_common_json_issues(json_str)
                try:
                    result = json.loads(fixed)
                except json.JSONDecodeError:
                    return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw_text": text[:500]}
            else:
                return {"error": "JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", "raw_text": text[:500]}

        if required_fields:
            missing = [f for f in required_fields if f not in result]
            if missing:
                result["_validation_warning"] = f"ëˆ„ë½ëœ í•„ë“œ: {missing}"

        return result

    def retry_with_backoff(
        self,
        func: Callable,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 10.0,
    ):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„"""
        last_error = None

        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    delay = min(base_delay * (2**attempt), max_delay)
                    logger.info(
                        f"ì¬ì‹œë„ {attempt + 1}/{max_retries} ({delay:.1f}ì´ˆ í›„)"
                    )
                    time.sleep(delay)

        raise last_error  # type: ignore
